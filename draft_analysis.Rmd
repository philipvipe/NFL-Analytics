---
title: 'NFL Draft Analysis'
author: "Philip Ipe"
date: "5/8/2020"
output: html_document
knit: (function(input_file, encoding) {
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), 'index.html'))})
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn = -1)
library(tidyverse)
library(rvest)
library(gridExtra)
library(caret)
library(knitr)
```

# Part 1: Data Collection

We will scrape historical draft data from Pro-Football Reference from years 1970 till 2016. We chose to stop at 2016 since rookies usually sign 4 year deals, so the four year mark is typically when each team will finish evaulating each player they drafted, and make a decision on whether or not to resign the player. So, the jury is still out on players drafted after 2016.  We will filter out any selections after round 7, as since 1994, the NFL draft has only consisted of 7 rounds. 

To scrape the data, we will use the *read_html()*, *html_node()*, and *html_table()* methods in the **rvest** package. More information on **rvest** is available here: https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/

First, we store the html file from our draft data website into the *draft_html* variable. From there, we search for the first occurence of the "table" element using *html_node()*, and then we parse the HTML data results into a dataframe using the *html_table()*. After parsing, we see that the column names are in the first row of the data, so we set the column names to be the first row of data with *names(df) <- as.character(unlist(df[1,]))*. Then we delete the first row of data (that still contains the column names) so all of the rows just contain player data.

Since each years draft data is on a different website, we loop through all of the website, and after some more processing (see note), we use *rbind()* to add each dataframe vertically to our *master_df* dataframe.

The result will be all of the 1970-2016 draft data stored in the dataframe *master_df* with entities for each player.

*Note: We will also do some process in this step as well, but we will go into more detail on that when we repeat the processing steps in another dataframe later in the walkthrough.
    
```{r scrape_master}

master_df <- data.frame()
for (year in 1970:2016){
  
  draft_url <- paste("https://www.pro-football-reference.com/years/", year, "/draft.htm", sep="")
  draft_html <- read_html(draft_url)

  df <- draft_html %>%
    html_node("table") %>%
    html_table()

  names(df) <- as.character(unlist(df[1,]))
  df <- df[-1,]
  
  # get rid of the columns we dont care about
  df <- df[,1:13]
  
  df$Rnd <- as.integer(df$Rnd)
  df$Age <- as.integer(df$Age)
  df$CarAV <- as.integer(df$CarAV)
  df$DrAV <- as.integer(df$DrAV)
  df$G <- as.integer(df$G)
  
  # filter out certain rounds
  df <- df %>%
    filter(Rnd < 8)
  
  # get rid of blank rows
  df <-df[!(df$Player=="Player"),]
  
  # add year drafted to each observation
  df <- df %>%
    mutate(year = year)
    
  master_df <- rbind(master_df, df)
}
```

We will also scrape the data from the 2020 draft in the same way for use in our machine learning problem and save it to our *df_2020* variable. 

```{r scrape_2020}

draft_url <- "https://www.pro-football-reference.com/years/2020/draft.htm"
draft_html <- read_html(draft_url)

df_2020 <- draft_html %>%
  html_node("table") %>%
  html_table()

names(df_2020) <- as.character(unlist(df_2020[1,]))
df_2020 <- df_2020[-1,]
```

We will also scrape the 2017-2019 NFL draft data into a seperate dataframe *test_df*, which will be used to test our machine learning model that we will develop later in the tutorial.

```{r scrape_test}
test_df <- data.frame()
for (year in 2017:2019){
  
  draft_url <- paste("https://www.pro-football-reference.com/years/", year, "/draft.htm", sep="")
  draft_html <- read_html(draft_url)

  df <- draft_html %>%
    html_node("table") %>%
    html_table()

  names(df) <- as.character(unlist(df[1,]))
  df <- df[-1,]
  
  # get rid of the columns we dont care about
  df <- df[,1:6]
  
  # add year drafted to each observation
  df <- df %>%
    mutate(year = year)

  test_df <- rbind(test_df, df)
}

```

Next we will scrape the AP Offensive Rookie of the Year Winners from the same website, again for our machine learning problem, and save into the *roty_df* dataframe.

```{r scrape_ROTY}

roty_url <- "https://www.pro-football-reference.com/awards/ap-offensive-rookie-of-the-year-award.htm"
draft_html <- read_html(roty_url)

roty_df <- draft_html %>%
  html_node("table") %>%
  html_table()
```

# Part 2: Data Processing and Preparation

Load the NFL draft history data from *master_df*, containing the draft data from rounds 1-7 from the 1970-2016 NFL draft. The first line in this chunk will ensure that we will only keep the observations that do not have any missing data entries. Then, we will convert the necessary columns to integers and factors (rather than chars). This way, we can use the columns values to group and to produce regression lines in our data visualization steps later.
    
```{r process_master}

master_df <- master_df[complete.cases(master_df[,12:13]),]
master_df$Rnd <- as.factor(master_df$Rnd)
master_df$Tm <- as.factor(master_df$Tm)
master_df$Pos <- as.factor(master_df$Pos)

master_df$Age <- as.integer(master_df$Age)
master_df$CarAV <- as.integer(master_df$CarAV)
master_df$DrAV <- as.integer(master_df$DrAV)
master_df$G <- as.integer(master_df$G)
master_df$Pick <- as.integer(master_df$Pick)

kable(head(master_df))
```

Our *master_df* dataframe now has entities for each player with the following variables:

    Rnd : Round in which the player was selected
    
    Pick: Overall selection in draft
    
    Tm  : Team
    
    Player: Player name
    
    Pos : Position
    
    Age
    
    To: Last year in the league
    
    AP1: First team All-Pro selections
    
    PB: Pro Bowl selections
    
    St: Number of years as primary starter for his team at his position
    
    CarAv: Weighted Career Approximate Value
    
                  = 100% of AV of his best season + 95% of AV of his next best season + 90% of AV of his third-best season + ...
                  
    DrAV: Average value accumulated for the team that drafted this player
    
    G : Games Played
    
    year: Year drafted

Now let's process our 2020 Draft data. We will be using the *filter()* and *mutate()* methods found in the **dplyr** package in the **tidyverse** package. More info on **dplyr** can be found here: https://dplyr.tidyverse.org/

First, we will get rid of columns we don't care about. 
Next, we only keep rounds 1-7 using *filter()* with the conditional that **Rnd** variable must be less than 8. 
There seem to be seperator rows between rounds that contain the column names again (probably for readability on the website), so we get rid of them by choosing to keep only rows where the **Player** variable does not equal "Player". 
We then add year drafted to each observation using *mutate()*, which allows us to add variables to an existing dataframe, and again use *filter()* to only include quarterbacks, wide receivers, and running backs. 
We noticed that some of the age data is missing for the players, so we fill in all of the missing age values with 22, since that is the most common age for a player to enter the pros.
We convert the necessary columns to integers and factors, all like we did for the historical draft dataset. 

```{r process2020}
# get rid of the columns we dont care about
df_2020 <- df_2020[,1:6]

# filter out certain rounds
df_2020 <- df_2020 %>%
  filter(Rnd < 8)
  
# get rid of seperator rows
df_2020 <-df_2020[!(df_2020$Player=="Player"),]
  
# add year drafted to each observation, and filter out the defensive players.
df_2020 <- df_2020 %>%
  mutate(year = year) %>%
  filter(Pos == "QB" | Pos == "WR" | Pos == "RB")

df_2020[is.na(df_2020)] <- 22

df_2020$Pick <- as.integer(df_2020$Pick)
df_2020$Age <- as.integer(df_2020$Age)
df_2020$Tm <- as.factor(df_2020$Tm)
df_2020$Pos <- as.factor(df_2020$Pos)

kable(head(df_2020))
```

So, now our *df_2020* dataframe contains only the offensive skill position players that were drafted in 2020 as entities. The variables of this dataframe are **Rnd**, **Pick**, **Tm**, **Player**, **Pos**, **Age**, and **Year** like we have in our *master_df* variable.

We will process the *test_df* dataframe in the same way as the *df_2020* dataframe.

```{r process_test}

# filter out certain rounds
test_df <- test_df %>%
  filter(Rnd < 8)
  
# get rid of blank rows
test_df <-test_df[!(test_df$Player=="Player"),]
  
# filter so dataframe only contains offensive skill position players.
test_df <- test_df %>%
  filter(Pos == "QB" | Pos == "WR" | Pos == "RB")

test_df[is.na(test_df)] <- 22

test_df$Pick <- as.integer(test_df$Pick)
test_df$Age <- as.integer(test_df$Age)
test_df$Tm <- as.factor(test_df$Tm)
test_df$Pos <- as.factor(test_df$Pos)

kable(head(test_df))
```

Now let's create a dataframe in which we can compare different positions that were drafted. First, we want to break up our data into 4 periods. We accomplish this with the *cut()* method on our **year** variable with parameter 'breaks' = 4. This will break up all of the data into 4 periods based on year, more information on cut is available here: https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/cut

We will use *mutate()* again to add a variable **period** for which period this entry is in.
Then for each period, we will take average career value of each position using *summarise()*, grouped by each round, period and position that the player was selected using *group_by()*. Both of these methods are also available in the **dplyr** package. 

We will make a new *pos_df* dataframe with these groupings.

```{r positions}
pos_df <- master_df %>%
  mutate(period = cut(year, breaks=4)) %>%
  group_by(period, Pos, Rnd) %>%
  summarise(Avg_AV = mean(CarAV))

kable(head(pos_df))
```

Let's also add a dataframe *off_df* that is the same as *master_df*, but contains a variable **roty** that is Yes when the player that was drafted from 1970-2016 was Offensive Rookie of the Year, and No otherwise. Again, since we are adding a new variable, we use *mutate()*, with a call to *ifelse()*. The condition in the ifelse will check if the entry's player name exists in the **Player** column of our rookie of the year table, if it exists it returns "Yes", and "No" otherwise. Lastly, we convert this new variable into a factor w/ 2 levels.

We will also filter the dataframe to only contain offensive skill position players, like we did for the 2020 draft data.

```{r off_roty}
off_df <- master_df %>%
  mutate(roty = ifelse(Player %in% roty_df$Player, "Yes", "No")) %>%
  filter(Pos == "QB" | Pos == "WR" | Pos == "RB")

off_df$roty <- as.factor(off_df$roty)

kable(head(off_df))
```

# Part 3: Exploratory Data Analysis

## Trends in Average Value

Our first visualization will answer the following questions for us:
Has average value for players selected in different rounds gone up over time?
Have GMs gotten better at drafting in the first round, second round, etc...?

To do this, we will plot the average value for all the players for each year since 1970, grouped by round. We will create our plots using **ggplot2** which is available in the **tidyverse** package, more info can be found here: https://ggplot2.tidyverse.org/


```{r avg_val_rd1}

avg_val_df <- master_df %>%
  group_by(year, Rnd) %>%
  summarise(Avg_AV = mean(CarAV))

avg_val_df %>%
  group_by(Rnd) %>%
  ggplot(mapping=aes(x=year, y=Avg_AV, color=Rnd)) +
    geom_point() +
    labs(x = "Year", y = "Average Career Value", title = "Average Value for Players Drafted in Different Rounds Over Time") + 
    geom_smooth(method=lm) 
```

Seems like GMs have not gotten better at drafting in any of the first four rounds. In fact, we see a slight decline in average value for all rounds over time. We do see that GMs are selecting better players in the earlier rounds, as generally the average value for the earlier rounds is consistently higher than the lower rounds throughout the years. We see the trends spread out as time goes on as well.

## Analyzing the Quarterback Position

Lets analyze the average values of different positions grouped by round and time period in which they were drafted. Let's start with the most important position in football.

So, lets create a bar graph. For each time period, we will graph the average value of a quarterback selected in each round.

```{r qb}
qb_bar <- pos_df %>%
  filter(Pos == "QB") %>%
  group_by(Rnd) %>%
  ggplot(mapping=aes(x=period, y=Avg_AV, fill=Rnd)) +
    geom_bar(stat="identity", position = "dodge") +
    labs(x = "Year", y = "Average Career Value", title = "Average Value for QBs Drafted in Different Rounds Over Different Time Periods")

qb_bar
```

In the past 20 years (2000-2020), it is much more likely to hit on a QB in the first round than in any of the other rounds. We see a huge drop in average value after the first round in the past 20 years. Average value dips in half after that, and continues to decrease until the 7th round. It is interesting to note that the 7th round averge value shoots up again to about the same average values of the 3rd and 4th round in the 2000s.

Before then, from 1990-2000, we see that the 6th round was a  value pick for QB, although Tom Brady might be the outlier that is skewing those numbers. But we also see that there was far more value in the second round QB selection from 1980-2000 than in the past 20 years.

Now let's focus our attention on the past 20 years and graph some visualizations of that data.

Our first graph for the 2000s will be the average value of a quarterback on the y-axis, and the pick number in which they were selected on the x-axis.

```{r qb2}

qb_av_pick <- master_df %>%
  filter(year >= 2000) %>%
  filter(Pos == "QB") %>%
  ggplot(mapping=aes(x=Pick, y=CarAV)) +
    geom_point()  +
    labs(x = "Pick", y = "Career Value", title = "Average Value over picks for QBs Drafted in the 2000s") + 
    geom_smooth(method='loess') + 
    geom_smooth(method=lm, color="green")

qb_av_pick
```

As we would expect the trend is that there is a decline in value until about pick 100, where the career value seems to be about the same from about pick 100-250. So the data suggests that selecting a QB at picks 100-150 would provide less bang for your buck, as you could likely get a player of the same quality with some of your later draft capital.

Our next plot will we a violin plot of the average value for quarterbakcs drafted in the 2000s in different rounds. This will give us a better idea of the distribution of the average value for the quarterback position.

```{r qb3}

qb_violin <- master_df %>%
  filter(year >= 2000) %>%
  filter(Pos == "QB") %>%
  ggplot(mapping=aes(x=factor(Rnd), y=CarAV)) +
    geom_violin()  +
    labs(x = "Rnd", y = "Career Value", title = "Average Value over Round for QBs Drafted in the 2000s") + 
    geom_smooth(method=lm)

qb_violin
```

As we would expect, the violin plots show us that most of the QBs selected in rounds 5-7 turn out to be busts, as most of the career values end up towards the bottom of the violin. 

However, the distributions reaffirm what we have seen before in our Round 1 QBs. The spread of the data is far greater in round 1, and decreases heavily in round 2, and from there the decreases in spread are less drastic, suggesting that you have more of a shot at getting a good player in round 1 than in any other round.

## Analyzing the WR and RB

Next we will provide the same graphs to provide similar insight into the RB and WR selections of the draft. We will use *grid.arrange* to show the graphs side by side in three columns.

It is interesting to note that the general trend has been to select RBs later in the draft, as RBs have always been considered as more expendable than a QB, and there seems to be a notion that you can always get a quality back in the later rounds, and that using first round capital on a RB is a poor management decision.

```{r rb_wr1}

rb_bar <- pos_df %>%
  filter(Pos == "RB" & period == "(2e+03,2.02e+03]") %>%
  group_by(Rnd) %>%
  ggplot(mapping=aes(x=period, y=Avg_AV, fill=Rnd)) +
    geom_bar(stat="identity", position = "dodge") +
    labs(x = "Year", y = "Average Career Value", title = "Average Value for RBs")

wr_bar <- pos_df %>%
  filter(Pos == "WR" & period == "(2e+03,2.02e+03]") %>%
  group_by(Rnd) %>%
  ggplot(mapping=aes(x=period, y=Avg_AV, fill=Rnd)) +
    geom_bar(stat="identity", position = "dodge") +
    labs(x = "Year", y = "Average Career Value", title = "Average Value for WRs")

qb_bar <- pos_df %>%
  filter(Pos == "QB" & period == "(2e+03,2.02e+03]") %>%
  group_by(Rnd) %>%
  ggplot(mapping=aes(x=period, y=Avg_AV, fill=Rnd)) +
    geom_bar(stat="identity", position = "dodge") +
    labs(x = "Year", y = "Average Career Value", title = "Average Value for QBs")

grid.arrange(qb_bar, rb_bar, wr_bar, ncol=3, top = "Average Value in Different Rounds in the 2000s")
```

Not as drastic of a difference in rounds 1 and 2 in the WR group. The RB average value in the 2000s seems to resemble the QB position.

```{r rb_wr2}

qb_av_pick <- master_df %>%
  filter(year >= 2000) %>%
  filter(Pos == "QB") %>%
  ggplot(mapping=aes(x=Pick, y=CarAV)) +
    geom_point()  +
    labs(x = "Pick", y = "Career Value", title = "QB Average Value") + 
    geom_smooth(method='loess') + 
    geom_smooth(method=lm, color="green")

rb_av_pick <- master_df %>%
  filter(year >= 2000 & Pos == "RB") %>%
  ggplot(mapping=aes(x=Pick, y=CarAV)) +
    geom_point()  +
    labs(x = "Pick", y = "Career Value", title = "RB Average Value") + 
    geom_smooth(method='loess') + 
    geom_smooth(method=lm, color="green")

wr_av_pick <- master_df %>%
  filter(year >= 2000 & Pos == "WR") %>%
  ggplot(mapping=aes(x=Pick, y=CarAV)) +
    geom_point()  +
    labs(x = "Pick", y = "Career Value", title = "WR Average Value") + 
    geom_smooth(method='loess') + 
    geom_smooth(method=lm, color="green")

grid.arrange(qb_av_pick, rb_av_pick, wr_av_pick, ncol=3, top = "Average Value Over Picks in the 2000s")

```

Much less steep of a drop off between round 1 to 2 in the WR group.

Again we see a trend that by pick 100 the value seems to hit a plateue.

```{r rb_wr3}

qb_violin <- master_df %>%
  filter(year >= 2000) %>%
  filter(Pos == "QB") %>%
  ggplot(mapping=aes(x=factor(Rnd), y=CarAV)) +
    geom_violin()  +
    labs(x = "Rnd", y = "Career Value", title = "QB Average Value") + 
    geom_smooth(method=lm)

rb_violin <- master_df %>%
  filter(year >= 2000) %>%
  filter(Pos == "RB") %>%
  ggplot(mapping=aes(x=factor(Rnd), y=CarAV)) +
    geom_violin()  +
    labs(x = "Rnd", y = "Career Value", title = "RB Average Value") + 
    geom_smooth(method=lm)

wr_violin <- master_df %>%
  filter(year >= 2000) %>%
  filter(Pos == "WR") %>%
  ggplot(mapping=aes(x=factor(Rnd), y=CarAV)) +
    geom_violin()  +
    labs(x = "Rnd", y = "Career Value", title = "WR Average Value") + 
    geom_smooth(method=lm)

grid.arrange(qb_violin, rb_violin, wr_violin, ncol=3, top = "Average Value over Round in the 2000s")
```

RBs in Round 1 always produce.
We again see that round 2 is a value round for WRs as it appears to resemble the type of value you get out of round 1.

# Part 4: Machine Learning

All of our machine learning goodness uses methods from the **caret** package, more info on this package can be found here: https://cran.r-project.org/web/packages/caret/vignettes/caret.html

## Predicting the Rookie Offensive MVP

Now, right off the bat, we hope that the model will predict that Joe Burrow has a higher likelihood of winning the MVP. Not only because he's a stud, but because he's fresh off the Heisman, and the first overall pick QB that is going to a team in desperate need of good QB play. Vegas has him as the clear favorite to win, and the season is set up for him to take the title, so if our model is worth anything, it should at the very least have Joe Burrow as one of its top predictions for the award.

First, lets control the resampling of our data. In this case, we’re going to cross-validate the data 3 times, therefore training the model 3 times on different portions of the data before settling on the best tuning parameters. For gbm, the tuning parameters are trees, shrinkage, and interaction depth.

```{r ml_control}

objControl <- trainControl(method='cv', number=3, returnResamp='none', summaryFunction = twoClassSummary, classProbs = TRUE)

```

## Creating our Model

Now to actually train the model...

First, we will set the predictor variables to **Tm**, **Pos**, and **Pick**, and the sole outcome variable as **roty**. Then we will use the *train()* method from the **caret** package to train the model using the *off_df* variable that contains all our historical draft data on all the offensive skill position players. We tell *train()* what predictor and outcome variables to use, as well as to use the gbm method with the resampling control that we set previously.

```{r train_model}

outcome_name <- 'roty'
predictor_names <- c('Tm', 'Pos', 'Pick')

set.seed(123)
model <- train(off_df[,predictor_names], off_df[,outcome_name], 
               method = "gbm",
               trControl = objControl,
               metric = "ROC",
               preProc = c("center", "scale"))
```

We will print the summary of the model below, which will display the relative influences of each of the predictor variables.

```{r sum_model}
summary(model)
```

The summaries tell us how much relative influence each variable had on the model. It was interesting to see that position had such absolutely zero influence in predicting the model. This tells us that quarterbacks, wide receivers, or running backs do not have a leg up on each other for the MVP award! 

What is even more interesting is that the team the player was drafted to seemed to influence the model more than the pick number itself! Before I had trained the model, I expected pick, position, and then team to have relative influence in that order.


## Testing against 2017-2019 data

Let's test our model against the 2017, 2018, and 2019 datasets. To do this, we will use the *predict()* function in the **caret** package and feed it the test data. Since we set parameter 'type' to "prob",  *predict()* will calculate probabilities that the model is sure about it choice that each observation from *test_df* will fall under the "Yes" and "No" outcome categories. So the **Yes** variable corresponds to the probability the model is sure about its choice that the player will become Rookie MVP, and **No** = probability the model is sure the player will not be the Rookie MVP.

```{r eval_test}

predictions <- predict(object=model, test_df[,predictor_names], type='prob')
kable(head(predictions))
```

Now let's join our prediction probabilities to the actual data, so we can look at the prediction probabilities for each player. We will use *merge*, which concatentaes dataframes horizontally. Setting the "by" parameter to "row.names" matches the dataframes by row index.

```{r join_probabilities_test}

test_df <- merge(test_df, predictions, by = "row.names", all.x = FALSE, all.y = FALSE)
test_df$No.x <- NULL
test_df$Row.names <- NULL

kable(head(test_df))
```

Now let's get the top 10 players that the model is most certain will be the Offensive Rookie of the Year for 2017, 2018, and 2019 respectively.

```{r top10_roty_test}

top10_roty_2017 <- test_df %>%
  filter(year == 2017) %>%
  top_n(10, Yes)

kable(top10_roty_2017)

top10_roty_2018 <- test_df %>%
  filter(year == 2018) %>%
  top_n(10, Yes)

kable(top10_roty_2018)

top10_roty_2019 <- test_df %>%
  filter(year == 2019) %>%
  top_n(10, Yes)

kable(top10_roty_2019)
```


The actual winners for Rookie Offensive MVP for 2017-2019 were Alvin Kamara, Saquon Barkley, and Kyler Murray (in that order). The model fails to list Kamara as a top 10 prediction for 2017, but lists Barkley as its fourth best prediction for 2018, and Murray as its top prediction for 2019!

## Predicting the 2020 Rookie Offensive MVP

Now let's use the model to predict the probabilities that each player from the 2020 draft will become Offensive Rookie of the Year.  Like we did for the testing portion, this code will give us the probability that the model is sure about its choice that the observation will fall under the "No" and "Yes" category.

```{r eval_2020}

predictions <- predict(object=model, df_2020[,predictor_names], type='prob')
```

Now let's join our prediction probabilities to the actual data like we did before.

```{r join_probabilities}

df_2020 <- merge(df_2020, predictions, by = "row.names", all.x = FALSE, all.y = FALSE)
df_2020$No.x <- NULL
df_2020$Row.names <- NULL

kable(head(df_2020))
```

Now let's get the top 10 players that the model is most certain will be the Offensive Rookie of the Year in 2020.

```{r top10_roty}

top10_roty <- df_2020 %>%
  top_n(10, Yes)

kable(top10_roty)
```

**Joe Burrow is our model's top prediction to win the MVP!!!** We also see that Cam Akers, Tua, Henry Ruggs III, Jerry Jeudy, CeDee Lamb, and Justin Jefferson also appear in the top 10. All of these players are among those that have the highest odds to win the the Offensive MVP according to OddsShark.

Again, it's important to highlight that we trained this model JUST based on the pick number, team, and position for each player, whereas the Vegas odds have most certainly taken into account the player's performance in college as well.